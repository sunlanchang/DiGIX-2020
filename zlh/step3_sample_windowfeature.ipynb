{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-09-17 01:16:58] - __init__.py[line:127] - INFO: Successfully Reload: /home/tione/notebook/huawei/cached_data/CACHE_data_step1_feature_0917.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from base import Cache\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('precision', 5)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('max_colwidth', 200)\n",
    "pd.set_option('display.width', 5000)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def reduce_mem(df, use_float16=False):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    tm_cols = df.select_dtypes('datetime').columns\n",
    "    for col in df.columns:\n",
    "        if col in tm_cols:\n",
    "            continue\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type).find('int') > -1:\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(\n",
    "                        np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(\n",
    "                        np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(\n",
    "                        np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(\n",
    "                        np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            elif str(col_type).find('float') > -1:\n",
    "                if use_float16 and c_min > np.finfo(\n",
    "                        np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(\n",
    "                        np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(\n",
    "        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "def multi_process_cal(cls, data, params, process_nums=3):\n",
    "    df_list = []\n",
    "    key = params['key']  # 根据key分组\n",
    "    data_group = data.groupby([key])\n",
    "    step = len(data_group.size()) // process_nums\n",
    "    index_list = []\n",
    "    for index, group in enumerate(data_group):\n",
    "        if index % step != 0:\n",
    "            index_list.append(group[1])\n",
    "        elif index % step == 0 and index != 0:\n",
    "            df_list.append([pd.concat(index_list), params])\n",
    "            index_list = [group[1]]\n",
    "        else:\n",
    "            index_list.append(group[1])\n",
    "    df_list.append([pd.concat(index_list), params])\n",
    "    with Pool(process_nums) as p:\n",
    "        result = p.map(cls, df_list)\n",
    "    fe = pd.concat(result)\n",
    "    fe = reduce_mem(fe, use_float16=True)\n",
    "    return fe\n",
    "\n",
    "\n",
    "def get_groupby_feature(df_list):\n",
    "    '''\n",
    "    # 做by 某个key的过去时间统计特征\n",
    "    :param df_list: list 参数1 groupby key的数据， 参数2 计算参数\n",
    "    :return: feature 主键为key\n",
    "    '''\n",
    "    # ctr平滑系数\n",
    "    alpha = 3\n",
    "\n",
    "    data, params = df_list[0].copy(), df_list[1]\n",
    "    key = params['key']\n",
    "    window_list = params['window']  # 过去几天的统计信息[1,8]\n",
    "    sparse_features = params['sparse_features']\n",
    "    dense_features = params['dense_features']\n",
    "    cols = ['index', 'pt_d'] + sparse_features + dense_features\n",
    "    if key not in cols:\n",
    "        cols = [key] + cols\n",
    "    febase = data[cols]\n",
    "    # data[['index', 'uid', 'task_id', 'adv_id', 'pt_d']]  # .drop_duplicates(subset=['index'])\n",
    "\n",
    "    sparse_aggfunc = ['count', 'nunique']\n",
    "    dense_aggfunc = ['max', 'min', 'mean', 'std', 'skew']\n",
    "\n",
    "    for window in tqdm(window_list):\n",
    "        # 只算过去一天\n",
    "        if window >= 1:\n",
    "            data['pt_d_last'] = data['pt_d'] + window\n",
    "            count_col = ''\n",
    "            # by key\n",
    "            for var in sparse_features:\n",
    "                for aggf in sparse_aggfunc:\n",
    "                    if len(count_col) > 0 and aggf == 'count':\n",
    "                        # 多列只算一次count\n",
    "                        continue\n",
    "                    else:\n",
    "                        # 用户昨天各个sparse 列的统计特征\n",
    "                        fe = data.groupby([key, 'pt_d_last'])[var].agg([aggf]).rename(\n",
    "                            columns={aggf: f'{key}_{var}_{window}_{aggf}'}).reset_index()\n",
    "                        fe.columns = [key, 'pt_d', f'{key}_{var}_{window}_{aggf}']\n",
    "                        febase = febase.merge(fe, on=[key, 'pt_d'], how='left')\n",
    "                        if aggf == 'count':\n",
    "                            count_col = f'{key}_{var}_{window}_{aggf}'  # key昨天曝光次数\n",
    "                # 多样性\n",
    "                febase[f'{key}_{var}_{window}_nunique_d_count'] = febase[f'{key}_{var}_{window}_nunique'] / febase[\n",
    "                    count_col]\n",
    "            febase[count_col] = febase[count_col].fillna(0)\n",
    "            for var in dense_features:\n",
    "                for aggf in dense_aggfunc:\n",
    "                    # key昨天各个dense 列的统计特征\n",
    "                    fe = data.groupby([key, 'pt_d_last'])[var].agg([aggf]).rename(\n",
    "                        columns={aggf: f'{key}_{var}_{window}_{aggf}'}).reset_index()\n",
    "                    fe.columns = [key, 'pt_d', f'{key}_{var}_{window}_{aggf}']\n",
    "                    febase = febase.merge(fe, on=[key, 'pt_d'], how='left')\n",
    "\n",
    "            # 可以补上label 计算过去一天内的ctr\n",
    "            # 昨天key总点击次数\n",
    "            fe = data.groupby([key, 'pt_d_last'])['label'].sum().rename(f'{key}_clicktimes_{window}').reset_index()\n",
    "            fe.columns = [key, 'pt_d', f'{key}_clicktimes_{window}']\n",
    "            febase = febase.merge(fe, on=[key, 'pt_d'], how='left')\n",
    "            febase[f'{key}_clicktimes_{window}'] = febase[f'{key}_clicktimes_{window}'].fillna(0)\n",
    "            # 用户昨天ctr\n",
    "            febase[f'{key}_ctr_{window}'] = febase[f'{key}_clicktimes_{window}'] / (febase[count_col] + alpha)\n",
    "\n",
    "            # 过去一天对今天行样本里的dense_feature的变化特征\n",
    "            for var in dense_features:\n",
    "                # 今天的值/昨天的值的均值，今天的值/昨天的值的最大值\n",
    "                febase[var + f'_d_{window}_mean'] = febase[var] / febase[f'{key}_{var}_{window}_mean']\n",
    "                febase[var + f'_d_{window}_max'] = febase[var] / febase[f'{key}_{var}_{window}_max']\n",
    "\n",
    "            # by key,var\n",
    "            # 过去一天对key对当前的var各个sparse_features的曝光率，点击率，ctr\n",
    "            for var in sparse_features:\n",
    "                # 昨天这项的曝光次数\n",
    "                fe = data.groupby([key, var, 'pt_d_last'])['label'].count().rename(\n",
    "                    f'{key}_{var}_curr_{window}').reset_index()\n",
    "                fe.columns = [key, var, 'pt_d', f'{key}_{var}_curr_{window}']\n",
    "                febase = febase.merge(fe, on=[key, var, 'pt_d'], how='left')\n",
    "                # 该项曝光占总曝光的比例\n",
    "                febase[f'{key}_{var}_curr_rate_{window}'] = febase[f'{key}_{var}_curr_{window}'] / (\n",
    "                        febase[count_col] + alpha)\n",
    "                # 昨天这项的总点击量\n",
    "                fe = data.groupby([key, var, 'pt_d_last'])['label'].sum().rename(\n",
    "                    f'{key}_{var}_clicktimes_{window}').reset_index()\n",
    "                fe.columns = [key, var, 'pt_d', f'{key}_{var}_clicktimes_{window}']\n",
    "                febase = febase.merge(fe, on=[key, var, 'pt_d'], how='left')\n",
    "                # 该项点击占总点击的比例\n",
    "                febase[f'{key}_{var}_clicktimes_rate_{window}'] = febase[f'{key}_{var}_clicktimes_{window}'] / (\n",
    "                        febase[f'{key}_clicktimes_{window}'] + alpha)\n",
    "                # 昨天这项的ctr\n",
    "                febase[f'{key}_{var}_ctr_{window}'] = febase[f'{key}_{var}_clicktimes_{window}'] / (\n",
    "                        febase[f'{key}_{var}_curr_{window}'] + alpha)\n",
    "\n",
    "    for var in ['uid', 'pt_d', 'task_id', 'adv_id'] + sparse_features + dense_features:\n",
    "        # 留index返回进行merge\n",
    "        if var in febase.columns:\n",
    "            del febase[var]\n",
    "    for var in febase.columns:\n",
    "        if var.find('count') > -1 or var.find('nunique') > -1 \\\n",
    "                or var.find('times') > -1 or var.find('curr') > -1 or var.find('ctr') > -1:\n",
    "            febase[var] = febase[var].fillna(0)\n",
    "    return febase\n",
    "\n",
    "data = Cache.reload_cache('CACHE_data_step1_feature_0917.pkl')\n",
    "\n",
    "params = {'key': 'uid',\n",
    "              'window': [1],\n",
    "              'sparse_features': ['task_id','creat_type_cd','adv_id','adv_prim_id','dev_id',\n",
    "                                  'inter_type_cd','spread_app_id','tags','app_first_class',\n",
    "                                  'app_second_class','indu_name','slot_id','net_type'],\n",
    "              'dense_features': ['app_score','his_app_size','his_on_shelf_time','device_size']}\n",
    "features_0 = multi_process_cal(get_groupby_feature, data, params, process_nums=20)\n",
    "features_0 = reduce_mem(features_0, use_float16=True)\n",
    "Cache.cache_data(features_0, nm_marker='data_step_3_features_0_0917')# 有index\n",
    "\n",
    "params = {'key': 'task_id',\n",
    "          'window': [1],\n",
    "          'sparse_features': ['uid','age','city','gender','device_name', 'residence','emui_dev',\n",
    "                             'slot_id','net_type','consume_purchase','career'],\n",
    "          'dense_features': ['city_rank','device_size','list_time','device_price','membership_life_duration',\n",
    "                             'communication_avgonline_30d','up_life_duration','up_membership_grade']}\n",
    "features_1 = multi_process_cal(get_groupby_feature, data, params, process_nums=20)\n",
    "features_1 = reduce_mem(features_1, use_float16=True)\n",
    "Cache.cache_data(features_1, nm_marker='data_step_3_features_1_0917')\n",
    "\n",
    "params = {'key': 'adv_id',\n",
    "          'window': [1],\n",
    "          'sparse_features': ['uid','age','city','gender','device_name', 'residence','emui_dev',\n",
    "                             'slot_id','net_type','consume_purchase','career'],\n",
    "          'dense_features': ['city_rank','device_size','list_time','device_price','membership_life_duration',\n",
    "                             'communication_avgonline_30d','up_life_duration','up_membership_grade']}\n",
    "features_2 = multi_process_cal(get_groupby_feature, data, params, process_nums=20)\n",
    "features_2 = reduce_mem(features_2, use_float16=True)\n",
    "Cache.cache_data(features_2, nm_marker='data_step_3_features_2_0917')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_py3",
   "language": "python",
   "name": "conda_tensorflow2_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from base import Cache\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('precision', 5)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('max_colwidth', 200)\n",
    "pd.set_option('display.width', 5000)\n",
    "\n",
    "def reduce_mem(df, use_float16=False):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    tm_cols = df.select_dtypes('datetime').columns\n",
    "    colsuse = [i for i in df.columns if i!= 'label']\n",
    "    for col in colsuse:\n",
    "        if col in tm_cols:\n",
    "            continue\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(\n",
    "                        np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(\n",
    "                        np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(\n",
    "                        np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(\n",
    "                        np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(\n",
    "                        np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(\n",
    "                        np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(\n",
    "        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "print('start!')\n",
    "data = Cache.reload_cache('CACHE_dataall0816.pkl')\n",
    "print(data.dtypes)\n",
    "data['communication_onlinerate'] = data['communication_onlinerate'].map(lambda x:x.replace('^',' '))\n",
    "route = Cache.reload_cache('CACHE_cmr0816.pkl')\n",
    "route_columns = [i for i in route.columns]\n",
    "data = pd.concat([data,route],axis=1)# 无index\n",
    "data = data.reset_index(drop=True).reset_index()# 添加index\n",
    "\n",
    "cols = [i for i in data.columns if i not in ['id','index']]\n",
    "data1= data.query('pt_d<8').drop_duplicates(subset=cols)# 重复样本去掉\n",
    "data2 = data.query('pt_d==8')\n",
    "\n",
    "def get_sample(df,day,total=1500000,rate=5):\n",
    "    set1 = df.query('pt_d=={}'.format(day))# 当日数据\n",
    "    set1_pos = set1.query('label==1')\n",
    "    nums_pos = set1_pos.shape[0]\n",
    "    nums_neg = nums_pos*rate\n",
    "    set1_neg = set1.query('label==0')\n",
    "    set1_neg = set1_neg.sample(nums_neg,random_state=0)# 剩余    \n",
    "    df_sample = pd.concat([set1_pos,set1_neg])\n",
    "    print(df_sample['label'].value_counts(),df_sample['label'].mean())\n",
    "    return df_sample\n",
    "\n",
    "data1_sample = []\n",
    "for day in [1,2,3,4,5,6,7]:\n",
    "    data1_sample.append(get_sample(data1,day))\n",
    "data1_sample = pd.concat(data1_sample)\n",
    "\n",
    "data = pd.concat([data1_sample,data2],ignore_index=True)\n",
    "data = data.sort_values(['uid','pt_d','slot_id','net_type','task_id','adv_id'],ascending=False).reset_index(drop=True)\n",
    "del data1_sample,data1,data2\n",
    "gc.collect()\n",
    "\n",
    "for var in route_columns:\n",
    "    data[var] = data[var].astype(int)\n",
    "print(data.dtypes)\n",
    "del route\n",
    "gc.collect()\n",
    "\n",
    "# 修正缺失值\n",
    "sparse_features=['task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id', 'dev_id', 'inter_type_cd', 'slot_id', 'spread_app_id', 'tags', 'app_first_class', 'app_second_class', 'city', 'device_name', 'career', 'gender', 'net_type', 'residence', 'emui_dev', 'indu_name', 'cmr_0', 'cmr_1', 'cmr_2', 'cmr_3', 'cmr_4', 'cmr_5', 'cmr_6', 'cmr_7', 'cmr_8', 'cmr_9', 'cmr_10', 'cmr_11', 'cmr_12', 'cmr_13', 'cmr_14', 'cmr_15', 'cmr_16', 'cmr_17', 'cmr_18', 'cmr_19', 'cmr_20', 'cmr_21', 'cmr_22', 'cmr_23', 'age', 'city_rank']\n",
    "dense_features=['his_app_size', 'his_on_shelf_time', 'app_score', 'device_size', 'list_time', 'device_price', 'up_life_duration', 'up_membership_grade', 'membership_life_duration', 'consume_purchase', 'communication_avgonline_30d', 'cmr_None']\n",
    "\n",
    "for var in sparse_features:\n",
    "    mode_num = data[var].mode()[0]\n",
    "    shape_null = data.query('{}==-1'.format(var)).shape[0]\n",
    "    print('process sparse int: ',var, 'fillna: ',mode_num, 'fillna_shape: ',shape_null)\n",
    "    if shape_null>0:\n",
    "        data.loc[data[var]==-1,var] = mode_num\n",
    "        data[var] = data[var].astype(int)\n",
    "    \n",
    "for var in dense_features:\n",
    "    mode_num = int(data[var].mean())\n",
    "    shape_null = data.query('{}==-1'.format(var)).shape[0]\n",
    "    print('process dense int: ',var, 'fillna: ',mode_num, 'fillna_shape: ',shape_null)\n",
    "    if shape_null>0:\n",
    "        data.loc[data[var]==-1,var] = mode_num\n",
    "        data[var] = data[var].astype(int)\n",
    "data = reduce_mem(data, use_float16=False)\n",
    "Cache.cache_data(data, nm_marker='data_step1_feature_0917')\n",
    "\n",
    "# ######################################################################################\n",
    "# base feature\n",
    "\n",
    "# 提取相对count特征\n",
    "## 列并行\n",
    "from multiprocessing import Pool\n",
    "\n",
    "cate_cols = ['task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id', 'dev_id', 'inter_type_cd', 'slot_id', 'spread_app_id', 'tags', 'app_first_class', 'app_second_class', 'city', 'device_name', 'career', 'gender', 'net_type', 'residence', 'emui_dev', 'indu_name', 'cmr_0', 'cmr_1', 'cmr_2', 'cmr_3', 'cmr_4', 'cmr_5', 'cmr_6', 'cmr_7', 'cmr_8', 'cmr_9', 'cmr_10', 'cmr_11', 'cmr_12', 'cmr_13', 'cmr_14', 'cmr_15', 'cmr_16', 'cmr_17', 'cmr_18', 'cmr_19', 'cmr_20', 'cmr_21', 'cmr_22', 'cmr_23', 'age']\n",
    "cate_cols_df = []\n",
    "for var in tqdm(cate_cols):\n",
    "    cate_cols_df.append(data[['uid','pt_d',var]])\n",
    "\n",
    "def cls(df):\n",
    "    f = df.columns[-1]\n",
    "    mapping = dict(df.query('pt_d<8')[f].value_counts()/df.query('pt_d<8')[f].value_counts().max())# 只统计train\n",
    "    mapping_test = dict(df.query('pt_d==8')[f].value_counts()/df.query('pt_d==8')[f].value_counts().max())# 只统计test\n",
    "    for key,value in mapping_test.items():\n",
    "        # 优先用train\n",
    "        if key not in mapping:\n",
    "            mapping[key]=value\n",
    "    df[f + '_count'] = df[f].map(mapping)# 映射\n",
    "    fe = df.groupby([f,'pt_d'])['uid'].count().rename(f'{f}_pt_d_count').reset_index()# 当天统计count\n",
    "    fe_max = fe.groupby('pt_d')[f'{f}_pt_d_count'].max().rename(f'{f}_pt_d_count_max').reset_index()\n",
    "    fe = fe.merge(fe_max,on='pt_d',how='left')\n",
    "    fe[f'{f}_pt_d_count']=fe[f'{f}_pt_d_count']/fe[f'{f}_pt_d_count_max']\n",
    "    fe[f'{f}_pt_d_count']=fe[f'{f}_pt_d_count'].fillna(0)\n",
    "    del fe[f'{f}_pt_d_count_max']\n",
    "    df = df.merge(fe,on = [f,'pt_d'],how='left')\n",
    "    print(df.columns)\n",
    "    return df[[f,'pt_d',f + '_count',f'{f}_pt_d_count']]\n",
    "\n",
    "with Pool(10) as p:\n",
    "    result = p.map(cls, cate_cols_df)\n",
    "for index,fe in enumerate(result):\n",
    "    f = cate_cols[index]\n",
    "    data = pd.concat([data,fe[fe.columns[-2:]]],axis=1)\n",
    "    print(fe.columns[-2:],f,data.shape)\n",
    "    del fe\n",
    "    gc.collect()\n",
    "del result,f,cate_cols_df\n",
    "gc.collect()\n",
    "data = reduce_mem(data, use_float16=False)\n",
    "\n",
    "# target_encoding\n",
    "\n",
    "##########################groupby feature#######################\n",
    "def group_fea(data,key,target):\n",
    "    tmp = data.groupby(key, as_index=False)[target].agg({\n",
    "        key+target + '_nunique': 'nunique',\n",
    "    }).reset_index()\n",
    "    del tmp['index']\n",
    "    return tmp\n",
    "\n",
    "def group_fea_pt_d(data,key,target):\n",
    "    tmp = data.groupby([key,'pt_d'], as_index=False)[target].agg({\n",
    "        key+target + '_pt_d_nunique': 'nunique',\n",
    "    }).reset_index()\n",
    "    fe = tmp.groupby('pt_d')[ key+target + '_pt_d_nunique'].max().rename('dmax').reset_index()\n",
    "    tmp = tmp.merge(fe,on='pt_d',how='left')\n",
    "    tmp[key+target + '_pt_d_nunique']=tmp[key+target + '_pt_d_nunique']/tmp['dmax']\n",
    "    del tmp['index'],tmp['dmax']\n",
    "    print(\"**************************{}**************************\".format(target))\n",
    "    return tmp\n",
    "\n",
    "feature_key = ['uid','age','gender','career','city','slot_id','net_type']\n",
    "feature_target = ['task_id','adv_id','dev_id','spread_app_id','indu_name']\n",
    "\n",
    "for key in tqdm(feature_key):\n",
    "    for target in feature_target:\n",
    "        tmp = group_fea(data,key,target)\n",
    "        data = data.merge(tmp,on=key,how='left')\n",
    "        tmp = group_fea_pt_d(data,key,target)\n",
    "        data = data.merge(tmp,on=[key,'pt_d'],how='left')\n",
    "del tmp\n",
    "gc.collect()\n",
    "data = reduce_mem(data, use_float16=False)\n",
    "\n",
    "test_df = data[data[\"pt_d\"]==8].copy().reset_index()\n",
    "train_df = data[data[\"pt_d\"]<8].reset_index()\n",
    "del data\n",
    "gc.collect()\n",
    "\n",
    "#统计做了groupby特征的特征\n",
    "group_list = []\n",
    "for s in train_df.columns:\n",
    "    if '_nunique' in s:\n",
    "        group_list.append(s)\n",
    "print(group_list)\n",
    "\n",
    "\n",
    "##########################target_enc feature#######################\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "enc_list = group_list + ['net_type','task_id','adv_id','adv_prim_id','age',\n",
    "                         'app_first_class','app_second_class','career','city','consume_purchase','uid','dev_id','tags','slot_id']\n",
    "for f in tqdm(enc_list):\n",
    "    train_df[f + '_target_enc'] = 0\n",
    "    test_df[f + '_target_enc'] = 0\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):\n",
    "        trn_x = train_df[[f, 'label']].iloc[trn_idx].reset_index(drop=True)\n",
    "        val_x = train_df[[f]].iloc[val_idx].reset_index(drop=True)\n",
    "        enc_df = trn_x.groupby(f, as_index=False)['label'].agg({f + '_target_enc': 'mean'})\n",
    "        val_x = val_x.merge(enc_df, on=f, how='left')\n",
    "        test_x = test_df[[f]].merge(enc_df, on=f, how='left')\n",
    "        val_x[f + '_target_enc'] = val_x[f + '_target_enc'].fillna(train_df['label'].mean())\n",
    "        test_x[f + '_target_enc'] = test_x[f + '_target_enc'].fillna(train_df['label'].mean())\n",
    "        train_df.loc[val_idx, f + '_target_enc'] = val_x[f + '_target_enc'].values\n",
    "        test_df[f + '_target_enc'] += test_x[f + '_target_enc'].values / skf.n_splits\n",
    "        \n",
    "del trn_x,val_x,enc_df,test_x\n",
    "gc.collect()\n",
    "# all features\n",
    "df_fe = pd.concat([train_df,test_df])\n",
    "del train_df,test_df\n",
    "df_fe = df_fe.sort_values('index').reset_index(drop=True)\n",
    "df_fe = reduce_mem(df_fe, use_float16=False)\n",
    "\n",
    "droplist = []\n",
    "set_test = df_fe.query('pt_d==8')\n",
    "for var in df_fe.columns:\n",
    "    if var not in ['id','index','label','pt_d']:\n",
    "        if set_test[var].nunique()<2 or set_test[var].count()<2:\n",
    "            droplist.append(var)\n",
    "print('drop list:',droplist)\n",
    "df_fe = df_fe.drop(droplist,axis=1)\n",
    "Cache.cache_data(df_fe , nm_marker='data_step2_feature_0917')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

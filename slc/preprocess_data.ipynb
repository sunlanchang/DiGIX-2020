{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_csv():\n",
    "    df_train = pd.read_csv('data/train_data.csv', sep='|', dtype=str)\n",
    "    df_test = pd.read_csv('data/test_data_A.csv', sep='|', dtype=str)\n",
    "    df_train = df_train.drop(columns=['label'])\n",
    "    for name in df_train.columns:\n",
    "        df_train[name] = df_train[name] + name\n",
    "\n",
    "    df_test = df_test.drop(columns=['id'])\n",
    "    for name in df_test.columns:\n",
    "        df_test[name] = df_test[name] + name\n",
    "    data = pd.concat([df_train, df_test], ignore_index=True)\n",
    "    data.to_csv('data/train_test.csv', sep=' ', index=0, header=0)\n",
    "# get_train_test_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding():\n",
    "    path = f'word2vec/word2vec_kv.kv'\n",
    "    wv = KeyedVectors.load(path, mmap='r')\n",
    "    feature_tokens = list(wv.vocab.keys())\n",
    "#     tokenizer = Tokenizer(num_words=1187210)\n",
    "    num_keys = len(list(wv.vocab.keys()))\n",
    "    print('number of keys', num_keys)\n",
    "    tokenizer = Tokenizer(num_words=num_keys,filters='\\n')#去除下划线\n",
    "    tokenizer.fit_on_texts(feature_tokens)\n",
    "    embedding_dim = 512\n",
    "    embedding_matrix = np.random.randn(\n",
    "        len(feature_tokens)+1, embedding_dim)\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = wv[word]\n",
    "        except:\n",
    "            print(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix\n",
    "# embedding_matrix = get_embedding()\n",
    "# np.save('word2vec/embedding_matrix.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_npy():\n",
    "    path = f'word2vec/word2vec_kv.kv'\n",
    "    wv = KeyedVectors.load(path, mmap='r')\n",
    "    feature_tokens = list(wv.vocab.keys())\n",
    "    num_keys = len(list(wv.vocab.keys()))\n",
    "    print('number of keys', num_keys)\n",
    "    tokenizer = Tokenizer(num_words=num_keys,filters='\\n')#去除_ ^\n",
    "    tokenizer.fit_on_texts(feature_tokens)\n",
    "    \n",
    "    feature_seq = []\n",
    "\n",
    "    with open('data/train_test.csv') as f:\n",
    "        for text in f:\n",
    "            feature_seq.append(text.strip())\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(feature_seq[:41907133])\n",
    "    x_train = pad_sequences(\n",
    "        sequences, maxlen=36, padding='post')\n",
    "    print(x_train.shape)\n",
    "    np.save('data/x_train.npy', x_train)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(feature_seq[41907133:])\n",
    "    x_test = pad_sequences(\n",
    "        sequences, maxlen=36, padding='post') \n",
    "    print(x_test.shape)\n",
    "    np.save('data/x_test.npy', x_test)\n",
    "# get_train_test_npy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_npy():\n",
    "    train = pd.read_csv('data/train_data.csv', sep='|', dtype=str)\n",
    "    label = train['label']\n",
    "    label = to_categorical(label)\n",
    "    np.save('data/label.npy',label)\n",
    "# get_label_npy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.load('data/label.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41907133, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit",
   "language": "python",
   "name": "python36564bit792083a9d155497086f5b8bc917c01d5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}